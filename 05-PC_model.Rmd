# Partial Credit Model {#PC_model}

This chapter provides a basic overview of the Partial Credit Model (PCM; [Masters, 1982] (https://link.springer.com/article/10.1007/BF02296272#citeas), along with guidance for analyzing data with the PCM using R. We use the same example data set from Chapter 4 of this book that includes participant responses to an attitude survey to illustrate the analysis using Marginal Maximum Likelihood Estimation (MMLE) and Joint Maximum Likelihood Estimation (JMLE). After the analyses are complete, we present an example description of the results. The chapter concludes with a challenge exercise.

## Partial Credit Model 

Masters (1982) proposed the PCM as a generalization of the dichotomous Rasch model (see Chapter 2) for use with ordinal item responses that are scored in more than two categories (e.g., data from attitude scales or performance assessments). Similar to the Rating Scale Model (RSM; Andrich, 1978; See chapter 4), the PCM provides estimates of *person locations*, *item locations*, and *rating scale category thresholds* on a log-odds scale that represents the latent variable. However, whereas the RSM provides one set of rating scale category thresholds that are estimated using all item responses, the PCM provides separate rating scale category thresholds for each item included in the analysis. Item-specific thresholds are useful in many practical situations, including instruments that include multiple scale lengths and cases where some rating scale categories are not observed in responses to one or more items. In addition, the PCM is useful in contexts where it is important to verify comparable rating scale functioning (see Chapter X) across items.

The PCM can be stated in log-odds form as follows:

$$ln\left[\frac{P_{n_i(xi=k)}}{P_{n_i(xi=k-1)}}\right]=\theta_{n}-\delta_{i}-\tau_{ik}$$

In the PCM, $\theta$ is the person's ability, $\delta$ is the item's difficulty, and $\tau_ik$ is the rating scale category threshold specific to item i. As in the PCM, the threshold is the location on the logit scale at which there is an equal probability for a rating in category *k* and category *k* - 1. For a rating scale made up of *m* categories, there are *m* - 1 rating scale category thresholds. Thresholds($τ_{k}$) are estimated empirically for each element of one facet, such as items. They are not necessarily evenly spaced or ordered as expected. In contrast to the RSM, the location and relative distance between thresholds is estimated separately for each item.

### Model Requirements
The PCM is based on the same requirements of unidimensionality, local independence, and invariance that we discussed in Chapter 2 for the dichotomous Rasch model. In practice, researchers should evaluate item responses for evidence that they approximate Rasch model requirements before examining model estimates in detail. Chapter 3 included details about model-data fit analysis procedures that can also be applied to the PCM. In the current chapter, we provide code for calculating some popular residual-based fit indices for items and persons based on the PCM. In addition, we encourage readers to use rating scale analysis techniques to consider additional issues related to rating scale functioning with the PCM; we discuss these methods in detail in Chapter XXX.

## R-Lab: Rasch Partial Credit Model with "eRm" package
For the Partial Credit Model, we will continue to work with the subset of the Braun (1988) essay data that we explored in the last Chapter. In this case, we will use "eRm" package.


### Model Requirements
Because it is a Rasch model, the RSM is based on the same requirements of unidimensionality, local independence, and invariance that we discussed in Chapter 2 for the dichotomous Rasch model. Evidence that rating scale responses approximate these requirements provides support for the meaningful interpretation and use of person, item, and threshold estimates on the logit scale as indicators of their respective on the latent variable. In practice, many analysts evaluate some or all of these requirements using various indicators of model-data fit. In the current chapter, we provide code for calculating some popular residual-based fit indices for items and persons. Readers can use the same techniques that we considered in Chapter 3 to evaluate fit to the RSM. In addition, readers can use rating scale analysis techniques to consider additional issues related to rating scale functioning; we discuss these methods in detail in Chapter XXX.

# Running the Partial Credit Model in R

In the next section, we provide a step-by-step demonstration of a PCM analysis using R. We encourage readers to use the example data set that is provided in the online supplement to conduct the analysis along with us.

## Example Data: Liking for Science
The example data for this chapter is the same data that we used in Chapter 4 of this book. The data include a group of 75 children's responses to the 25-item *Liking for Science* questionnaire, which was designed to measure their attitudes toward science activities. The data were published in Wright and Masters (1982). Each item stem included a science activity, and three response options: 0 = *Dislike*, 1 = *Not Sure/Don't Care*, and 2 = *Like*, such that responses in higher categories indicated more-favorable attitudes toward science activities.

## Prepare for the Analyses: Install and Load Packages
We will use the *Extended Rasch Modeling*, or *eRm* package (Mair et al., 2020) as the first package with which we demonstrate PCM analyses in this chapter. We selected eRm for the first illustrations in the current chapter because it includes functions for applying the PCM that are relatively straightforward to use and interpret. Please note that the "eRm" package  uses the Conditional Maximum Likelihood Estimation (CMLE) method to estimate Rasch model parameters. As a result, estimates from the eRm package are not directly comparable to estimates obtained using other estimation methods. At the end of this chapter, we have included an illustration of RSM analyses with the *Test Analysis Modules* or TAM package (Robitzsch et al., 2020) with Marginal Maximum Likelihood Estimation (MMLE). We also provide an illustration with TAM using Joint Maximum Likelihood Estimation (JMLE), which produces comparable estimates to some popular standalone Rasch software programs, such as Winsteps (Linacre, 2020a) and Facets (Linace, 2020b). 

To get started with the eRm package, view the citation information, and then install and load it into your R environment using the following code:

```{r}
citation("eRm")
# install.packages("eRm")
#install.packages("eRm")
library("eRm")
```

## Getting Started

Now that we have installed and loaded the packages to our R session, we are ready to import the data. We will use the function *read.csv()* to import the comma-separated values (.csv) file that contains the Liking for Science survey data. We encourage readers to use their preferred method for importing data files into R or R Studio.

Please note that if you use *read.csv()* you will need to specify the file path to the location at which the data file is stored on your computer or set your working directory to the folder in which you have saved the data.

First, we will import the data using *read.csv()* and store it in an object called *science*:

```{r}
science <- read.csv("liking_for_science.csv")
```

Next, we will explore the data using descriptive statistics using the *summary()* function:

```{r}
summary(science)
```

From the summary of *science*, we can see there are no missing data. We can also get a general sense of the scales, range, and distribution of each variable in the data set.

We can see that Student ID numbers range from 1 to 75, and that the maximum rating on the items was *x* = 2. Importantly, we can see that for item 12, the minimum rating was *x* = 1, and no ratings in the first category (*x* = 0) were observed in our sample. In contrast to the RSM analysis in Chapter 4, we can include this item in our analysis with the PCM because the PCM estimates rating scale category thresholds separately for each item. 

## Run the Partial Credit Model

Next we need to isolate the item response matrix from the descriptive variables in the data (in this case, student IDs). To do this, we will create an object made up of only the item responses by removing the first variable (*student*) from the data:

```{r}
# Remove the Student ID variable:
science.responses <- subset(science, select = -student)
```

We will use *summary()* to calculate descriptive statistics for the *science.responses* object to check our work and ensure that the responses are ready for analysis:

```{r}
# Check descriptive statistics:
summary(science.responses)
```

Now, we are ready to run the PCM on the Liking for Science response data. We will use the *PCM()* function to run the model and store the results in an object called *PCM.science*. Then, we will request a summary of the model results using the *summary()* function:
```{r}
PCM.science <- PCM(science.responses, se = TRUE)
summary(PCM.science)
```
When we run the PCM on the science data, we see a message indicating that the responses for item 12 have been shifted such that the lowest category is equal to zero. We need to keep this in mind when we interpret the threshold estimates for item 12. 

The summary of the PCM output includes the Conditional Log-likelihood statistic, details about the number of iterations and model parameters, and a table with item parameters, their standard errors, and confidence intervals. It is important to note that the item parameters included in this preliminary output are *item easiness* parameters--*not* item difficulty parameters. We will examine item difficulty parameters in detail later in our analysis.

# Plot a Wright Map to visualize item and person locations:

Next, we will create a Wright Map from our model results. This display will provide us with an overview of the distribution of person, item, and threshold parameters. We will create the plot using the eRm package function *plotPImap* on the model object (*PCM.science*). 
```{r}
plotPImap(PCM.science, main = "Liking for Science Partial Credit Model Wright Map")
```

In this *Wright Map* display, the results from the PCM analysis of the Liking for Science data are summarized graphically. The figure is organized as follows:

Starting at the bottom of the figure, the horizontal axis (labeled *Latent Dimension*) is the logit scale that represents the latent variable. In the application of the Liking for Science data, lower numbers indicate less-favorable attitudes toward science activities, and higher numbers indicate more-favorable attitudes toward science activities. 

The central panel of the figure shows item difficulty locations on the logit scale for the 25 Liking for Science items; the y-axis for this panel shows the item labels. By default in eRm, the items are ordered according to their original order in the response matrix. The items can be ordered by difficulty by adding "sorted = TRUE" as an argument in the *plotPImap()* call. For each item, a solid circle plotting symbol shows the overall location estimate. This solid circle symbol is connected to two open-circle symbols that show the locations of the rating scale category thresholds. Each threshold is labeled with a "1" to indicate the threshold between rating scale category *x* = 0 and *x* = 1, or a "2" to indicate the threshold between rating scale category *x* = 1 and *x* = 2. Because the PCM estimates thresholds separately for each item, the distance between the thresholds varies across items. In addition, an asterisk symbol (*) is shown to the right of the central panel of the Wright map that corresponds to item 13. For this item, the thresholds were disordered.

The upper panel of the figure shows a histogram of person (in this case, children) location estimates on the logit scale. Small vertical lines on the x-axis of this histogram show the points on the logit scale at which information (essentially variance) is maximized for the sample of persons and items in the analysis. These lines can be omitted by adding "irug = FALSE" as an argument in the *plotPImap()* call.

## Examine item parameters

Next, we will examine the item parameter estimates. In the PCM, item difficulty and threshold locations are combined. The eRm package reports item-specific rating scale category threshold parameters for each item as part of the *PCM()* function that we used to estimate the model. We extract these parameters, print them to the console, and calculate summary statistics for them with the following code:

```{r}
item.locations <- PCM.science$etapar

item.locations
summary(item.locations)
```

Because of the nature of the estimation process used in eRm, the item.locations object that we just created does not include the location estimate for the first threshold for the first item. One can calculate the location for item 1 by subtracting the sum of the item parameter estimates from zero. In the following code, we find the location for item 1 threshold 1, and then create a new object with all of the item + threshold locations:
```{r}
i1 <- 0 - sum(item.locations[(1:length(item.locations)) - 1])
              
item.locations.all <- c(i1, item.locations[(1:length(item.locations)) - 1])
item.locations.all
```

Alternatively, one can apply the *thresholds()* function to the model object in order to find the item locations from the PCM. This procedure provides item location estimates ($\delta\) as well as estimates of the item location combined with rating scale category thresholds ($\delta\ + $\tau\). However, the values produced with this function are not centered at zero logits, so a little manipulation is required to obtain the centered values. In the following code chunk, we apply the *thresholds()* function to obtain the uncentered item location estimates and then calculate centered item locations:

```{r}
# Apply thresholds() function to the model object in order to obtain item locations (not centered at zero logits):
items.and.taus <- thresholds(PCM.science)
items.and.taus.table <- as.data.frame(items.and.taus$threshtable)
uncentered.item.locations <- items.and.taus.table$X1.Location

# set the mean of the item locations to zero logits:
centered.item.locations <- scale(uncentered.item.locations, scale = FALSE)
summary(centered.item.locations)
```

It is important to note that the eta parameters from the PCM object include *cumulative* rating scale category thresholds. Cumulative thresholds are not typically used in Rasch model applications; Rasch-Andrich (i.e., adjacent categories) thresholds are typically used instead (Andrich, 2015; Mellenbergh, 1995). We will calculate the values of the rating scale category thresholds using the results from the *thresholds()* function. 

In our example, we have a rating scale with three categories, so we have two rating scale category thresholds for all of the items in which all categories were observed. In the following code chunk, we create an empty object in which to store the threshold estimates (*tau.matrix*), and then we use a for-loop to calculate the estimates and store them in our object. 

```{r}
# Specify the number of items that were included in the analysis:
n.items <- ncol(science.responses)

# Specify the number of thresholds as the maximum observed score in the response matrix (be sure the responses begin at category 0):
n.thresholds <- max(science.responses)

# Create a matrix in which to store the adjacent-category threshold values for each item:
tau.matrix <- matrix(data = NA, ncol = n.thresholds, nrow = n.items)


# Calculate adjacent-category threshold values:

for(item.number in 1:n.items){
  
  for(tau in 1:n.thresholds){
    tau.matrix[item.number, tau] <- (items.and.taus.table[item.number, (1+tau)] -    
                                       items.and.taus.table[item.number,1])[1]
  }
}
```

Next, we will calculate standard errors for each item + threshold location and store them in an object called *delta.tau.se*. We will use *summary()* to examine descriptive statistics for each item + threshold standard error.

```{r}
#SE for items + thresholds:
delta.tau.se <- items.and.taus$se.thresh
summary(delta.tau.se)
```

### Plot item response functions (rating scale category probability plots)

We will examine graphical displays of item difficulty using item response functions. With the PCM, the eRm package creates plots of the probability for a rating in each rating scale category, conditional on person locations on the latent variable. In the following code, we use *plotICC()* from eRm to create rating scale category probability plots for the 25 items in our analysis. We included "ask = FALSE" in our function call in order to generate all of the plots at once.

```{r}
plotICC(PCM.science, ask = FALSE)
```
This code generates plots of rating scale category probabilities for each item. In each item-specific plot, the x-axis is the logit scale that represents the latent variable; this scale represents favorability toward science activities in our example. The y-axis is the probability for a rating in each category, conditional on person locations on the latent variable. Separate lines in different colors show the conditional probability for each category of responses observed for the item of interest. Because we used the PCM, the overall shape of the curves and the relative distance between the curves is unique for each items--reflecting the individual set of rating scale category thresholds for each item. In addition, the location of the curves on the x-axis shifts to reflect each item's overall difficulty level.

#### Note about item fit

In the eRm package, it is necessary to calculate person parameters *before* item fit statistics can be calculated. Accordingly, we will proceed with a brief examination of person parameters before we conduct item fit analyses. In practice, we recommend examining item fit before examining and interpreting item locations in detail.

### Examine person parameters
The next step in our analysis is to calculate and examine person location parameters (i.e., person achievement or ability estimates). With the CMLE method that is used in eRm, person parameters are calculated *after* the item locations are estimated.

In the following code, we calculate person locations that correspond to our model using the *person.parameter()* function with the PCM model object (*PCM.science*). This function also produces standard errors for the person locations. We stored the person location estimates and their standard errors in a new data frame called *person.locations*, and then requested a summary of the estimation results using *summary()*:

```{r}
# Calculate person parameters:
person.locations.estimate <- person.parameter(PCM.science)

# Store person parameters and their standard errors in a dataframe object:
person.locations <- cbind.data.frame(person.locations.estimate$thetapar,
                                     person.locations.estimate$se.theta)
names(person.locations) <- c("theta", "SE")

# View summary statistics for person parameters:
summary(person.locations)
```

The estimation procedure in eRm does not directly produce parameter estimates for persons with extreme scores. In our example, extreme scores would result from a child giving a response of *x* = 0 to all items or a child giving a response of *x* = 2 to all items. For these students, a standard error is not calculated. In our example, Child 2 had an extreme score because they gave a rating in category 2 to all items.

### Examine item fit statistics
Next, we will conduct a brief exploration of item fit statistics for the Liking for Science items. We considered item fit in detail in Chapter 3; readers can use the same procedures in that chapter to examine item fit in detail for the PCM.

To calculate numeric item fit statistics, we will use the function *itemfit()* from eRm on the person parameter object (*person.locations.estimate*). This function produces several item fit statistics, including infit mean square error (MSE), outfit MSE, and standardized infit and outfit MSE statistics. We will store the item fit results in a new object called *item.fit*, and then format this object as a dataframe for easy manipulation and exporting:
```{r}
item.fit.results <- itemfit(person.locations.estimate)
item.fit <- cbind.data.frame(item.fit.results$i.infitMSQ,
                             item.fit.results$i.outfitMSQ,
                             item.fit.results$i.infitZ,
                             item.fit.results$i.outfitZ)
names(item.fit) <- c("infit_MSE", "outfit_MSE", "std_infit", "std_outfit")
```

Next, we will request a summary of the numeric item fit statistics using the *summary()* function:
```{r}
summary(item.fit)
```

The *item.fit* object includes mean square error (*MSE*) and standardized (*Z*) versions of the Outfit and Infit statistics for each item included in the analysis.These statistics are summaries of the residuals associated with each item. When data fit Rasch model expectations, the MSE versions of Outfit and Infit are expected to be close to 1.00 and the standardized versions of Outfit and Infit are expected to be around 0.00. Please refer to Chapter 3 for a more-detailed discussion of item fit.

### Examine person fit statistics
Next, we will conduct a brief exploration of person fit statistics. To calculate numeric person fit statistics, we will use the function *personfit()* from eRm on the person parameter object (*person.locations.estimate*). This function produces several person fit statistics, including infit mean square error (MSE), outfit MSE, and standardized infit and outfit MSE statistics. We will store the person fit results in a new object called *person.fit*, and then format this object as a dataframe for easy manipulation and exporting:
```{r}
person.fit.results <- personfit(person.locations.estimate)
person.fit <- cbind.data.frame(person.fit.results$p.infitMSQ,
                             person.fit.results$p.outfitMSQ,
                             person.fit.results$p.infitZ,
                             person.fit.results$p.outfitZ)
names(person.fit) <- c("infit_MSE", "outfit_MSE", "std_infit", "std_outfit")
```

Next, we will request a summary of the numeric person fit statistics using the *summary()* function:
```{r}
summary(person.fit)
```

The *person.fit* object includes mean square error (*MSE*) and standardized (*Z*) versions of the Outfit and Infit statistics for each person. These statistics are summaries of the residuals associated with each person. When data fit Rasch model expectations, the MSE versions of Outfit and Infit are expected to be close to 1.00 and the standardized versions of Outfit and Infit are expected to be around 0.00. Please refer to Chapter 3 for a more-detailed discussion of person fit.

## Summarize the results in tables
As a final step, we will create tables that summarize the calibrations of the persons, items, and rating scale category thresholds.

Table 1 is an overall model summary table that provides an overview of the logit scale locations, standard errors, fit statistics, and reliability statistics for items and persons. This type of table is useful for reporting the results from Rasch model analyses because it provides a quick overview of the location estimates and numeric model-data fit statistics for the items and persons in the analysis.

### Model summary table:
-[] Cheng - can you please add reliability of separation statistics to these tables??
```{r}
PCM_summary.table.statistics <- c("Logit Scale Location Mean",
                              "Logit Scale Location SD",
                              "Standard Error Mean",
                              "Standard Error SD",
                              "Outfit MSE Mean",
                              "Outfit MSE SD",
                              "Infit MSE Mean",
                              "Infit MSE SD",
                              "Std. Outfit Mean",
                              "Std. Outfit SD",
                              "Std. Infit Mean",
                              "Std. Infit SD")
                              
PCM_item.summary.results <- rbind(mean(centered.item.locations),
                              sd(centered.item.locations),
                              mean(delta.tau.se),
                              sd(delta.tau.se),
                              mean(item.fit.results$i.outfitMSQ),
                              sd(item.fit.results$i.outfitMSQ),
                              mean(item.fit.results$i.infitMSQ),
                              sd(item.fit.results$i.infitMSQ),
                              mean(item.fit.results$i.outfitZ),
                              sd(item.fit.results$i.outfitZ),
                              mean(item.fit.results$i.infitZ),
                              sd(item.fit.results$i.infitZ))

PCM_person.summary.results <- rbind(mean(person.locations$theta),
                              sd(person.locations$theta),
                              mean(person.locations$SE),
                              sd(person.locations$SE),
                              mean(person.fit$outfit_MSE),
                              sd(person.fit$outfit_MSE),
                              mean(person.fit$infit_MSE),
                              sd(person.fit$infit_MSE),
                              mean(person.fit$std_outfit),
                              sd(person.fit$std_outfit),
                              mean(person.fit$std_infit),
                              sd(person.fit$std_infit))

# Round the values for presentation in a table:
PCM_item.summary.results_rounded <- round(PCM_item.summary.results, digits = 2)

PCM_person.summary.results_rounded <- round(PCM_person.summary.results, digits = 2)

PCM_Table1 <- cbind.data.frame(PCM_summary.table.statistics,
                           PCM_item.summary.results_rounded, 
                           PCM_person.summary.results_rounded)

# add descriptive column labels:
names(PCM_Table1) <- c("Statistic", "Items", "Persons")  
```

### Item calibration table:
Table 2 is a table that summarizes the overall calibrations of individual items. For data sets with manageable sample sizes such as the Liking for Science data example in this chapter, we recommend reporting details about each item in a table similar to this one.

```{r}
# Calculate the average rating for each item:
Avg_Rating <- apply(science.responses, 2, mean)

# Combine item calibration results in a table:

PCM_Table2 <- cbind.data.frame(c(1:ncol(science.responses)), 
                           Avg_Rating,
                           centered.item.locations,
                           tau.matrix,
                           item.fit$outfit_MSE,
                           item.fit$std_outfit,
                           item.fit$infit_MSE,
                           item.fit$std_infit)
names(PCM_Table2) <- c("Task ID", "Average Rating", "Item Location","Threshold 1", "Threshold 2", "Outfit MSE","Std. Outfit", "Infit MSE","Std. Infit")                           

# Sort Table 2 by Item difficulty:
PCM_Table2 <- PCM_Table2[order(-PCM_Table2$`Item Location`),]

# Round the numeric values (all columns except the first one) to 2 digits:
PCM_Table2[, -1] <- round(PCM_Table2[,-1], digits = 2)

```

### Person calibration table:
Finally, Table 3 provides a summary of the person calibrations. When there is a relatively large person sample size, it may be more useful to present the results as they relate to individual persons or subsets of the person sample as they are relevant to the purpose of the analysis.

In our person calibration table, we have included the results for all of the children with non-extreme scores. This includes all of the children in our sample except Child 2.

```{r}
# Calculate average rating for persons who did not have extreme scores
Person_Avg_Rating <- apply(person.locations.estimate$X.ex,1, mean)

# Combine person calibration results in a table:
PCM_Table3 <- cbind.data.frame(rownames(person.locations),
                          Person_Avg_Rating,
                          person.locations$theta,
                          person.locations$SE,
                          person.fit$outfit_MSE,
                          person.fit$std_outfit,
                          person.fit$infit_MSE,
                          person.fit$std_infit)
                          
names(PCM_Table3) <- c("Child ID", "Average Rating", "Person Location","Person SE","Outfit MSE","Std. Outfit", "Infit MSE","Std. Infit")

# Round the numeric values (all columns except the first one) to 2 digits:
PCM_Table3[, -1] <- round(PCM_Table3[,-1], digits = 2)

```



# PCM Application with Marginal Maximum Likelihood Estimation in TAM
The next section of this chapter includes an illustration of PCM analyses with the *Test Analysis Modules* or TAM package (Robitzsch et al., 2020) with Marginal Maximum Likelihood Estimation (MMLE). After this illustration, we also demonstrate the use of TAM to estimate the PCM with Joint Maximum Likelihood Estimation (JMLE). These illustrations use the Liking for Science data set that we described earlier in this chapter.

Except where there are significant differences between the eRm and TAM procedures, we provide fewer details about the analysis procedures and interpretations in this section compared to the first illustration.

## Getting Started

To get started with the TAM package, view the citation information, and then install and load it into your R environment using the following code:

```{r}
citation("TAM")
# install.packages("TAM")
#install.packages("TAM")
library("TAM")
```

To facilitate the example analysis, we will also use the *WrightMap* package (Torres Irribarra & Freund, 2014):
  
WrightMap:
```{r}
citation("WrightMap")
# install.packages("WrightMap")
library("WrightMap") 
```

## Prepare the data for analysis

If you have not already imported the Liking for Science data and prepared it for analysis as described earlier in this chapter by dropping item 12 and isolating the response matrix, please do so before continuing with the TAM analyses.  

## Run the Rating Scale Model

In order to obtain Rasch-Andrich thresholds (i.e., adjacent-categories thresholds) from our analysis, we need to generate a design matrix for the model that includes specifications for those parameters. We will do this using the *designMatrices()* function from TAM and save the result in a new object called *design.matrix*:
```{r}
design.matrix <- designMatrices(resp=science.responses, modeltype="PCM", constraint = "items")$A
```

Now we can run the RSM with our design matrix using the *tam.mml()* function with several specifications. After we run the model, we will request a summary of the model results using the *summary()* function.
-[] omit the model run output, keep the summary output.
```{r}
PCM.science_MMLE <- tam.mml(science.responses, irtmodel="PCM", A = design.matrix, constraint = "items") 

summary(PCM.science_MMLE)
```

The summary of the RSM output includes details about the number of iterations, global model fit statistics, a summary of the model parameters, and several other statistics. 

## Examine item parameters

Next, we will examine the item difficulty location and rating scale category threshold estimates. As of this writing, the TAM package does not provide centered item estimates (mean set to zero logits) with the design matrix that we specified in our analysis. As a result, we need to manually center the item locations at zero logits for ease in interpretation. We will use the same procedure that we used earlier to do the centering.

First, we need to extract the item location estimates from the model object (*PCM.science_MMLE*). The item locations are labeled as *xsi* parameters, and they include rating scale category thresholds after the overall item parameters are given. We will save the overall item location parameter estimates and their standard errors for our 24 Liking for Science Items by selecting the first 24 rows of the xsi results in an object called *items_MMLE*.

```{r}
items_MMLE <- PCM.science_MMLE$xsi[1:(ncol(science.responses)),]
```

Next, we will center the item parameter estimates that are stored in *items_MMLE* at zero logits, and request summary statistics for the estimates to check our work:

```{r}
uncentered.item.locations_MMLE <- items_MMLE$xsi
centered.item.locations_MMLE <- scale(uncentered.item.locations_MMLE, scale = FALSE)
summary(centered.item.locations_MMLE)
```

We need to find the rating scale category threshold parameter estimates for our rating scale. In our example with the Liking for Science data, our rating scale has three categories, so there are two threshold parameters.
The following code extracts the adjacent-categories threshold parameters from the item parameter table that is stored in the *PCM.science_MMLE* object, and then stores them in an object called *tau.estimates_MMLE*:
```{r}
# View the item parameter table (note that the overall item location shown here is not centered):
PCM.science_MMLE$item_irt

# Save only the threshold estimates, which begin in column 4:
tau.estimates_MMLE <- PCM.science_MMLE$item_irt[, c(4 : (3+n.thresholds))]

# View the threshold estimates:
tau.estimates_MMLE
```
### Examine item response functions (rating scale category probability plots)

Next, we will examine rating scale category probability plots for the 25 items in our analysis.

```{r}
#graphics.off()
plot(PCM.science_MMLE, type="items")
```

This code generates plots of rating scale category probabilities for each item. These plots have the same interpretation as the rating scale category probability plots that we generated using eRm, where the x-axis is the logit scale that represents the latent variable, the y-axis is the probability for a rating in each category, and individual lines show the conditional probability for a rating in each category.

## Evaluate item fit
Next, we will examine numeric and graphical item fit indices using the *itemfit()* function from TAM. We will save the results in an object called *item.fit_MMLE* and then view summary statistics for the fit statistics.

-[] omit the MMLE_fit output, keep the summary output.
```{r}
MMLE_fit <- tam.fit(PCM.science_MMLE)

item.fit_MMLE <- MMLE_fit$itemfit
summary(item.fit_MMLE)
```

As in the dichotomous Rasch model analysis with TAM (see Chapter 2), the *tam.fit()* function provides mean square error (*MSE*) and standardized (*t*) versions of the Outfit and Infit statistics for Rasch models. The *Outfit* and *Infit* statistics are the *MSE* versions and the *Outfit_t* and *Infit_t* statistics are the standardized versions of the statistics. TAM also reports a *p* value for the standardized fit statistics (*Outfit_p* and *Infit_p*), along with adjusted significance values (*Infit_pholm* and *Outfit_pholm*). Please see Chapter 3 for a detailed consideration of procedures for evaluating item fit in detail, including the use of graphical tools to evaluate item fit.

## Examine person parameters

Now we will examine person parameter estimates. With the MMLE procedure in TAM, person parameters are calculated after the item estimates using the *tam.wle()* function. The following code calculates person parameter estimates and saves them in an object called person.locations_MMLE.

```{r}
# Use the tam.wle function to calculate person location parameters:
person.locations.estimate_MMLE <- tam.wle(PCM.science_MMLE)

# Store person parameters and their standard errors in a dataframe object:
person.locations_MMLE <- cbind.data.frame(person.locations.estimate_MMLE$theta,
                                     person.locations.estimate_MMLE$error)

names(person.locations_MMLE) <- c("theta", "SE")

# View summary statistics for person parameters:
summary(person.locations_MMLE)
```

Because we centered the item location estimates at zero logits for our interpretation earlier, we need to adjust the person location estimates so that they can be compared to the centered item locations. We will do this by subtracting the original (uncentered) item mean from the person locations.

```{r}
# Subtract the original (uncentered) item mean location from the person locations:
person.locations_MMLE$theta_adjusted <- person.locations_MMLE$theta - mean(uncentered.item.locations_MMLE)

# Summary of person location estimates:
summary(person.locations_MMLE)
```

If analysts do not prefer to use the zero-centered item location estimates, they can use the original person parameters without subtracting the mean item location.

## Evaluate person fit
We can evaluate person fit using the *tam.personfit()* function from TAM. This function uses the model object as an argument and it produces infit and outfit statistics, as well as standardized versions of these statistics, for each person in the response matrix.
```{r}
person.fit.results_MMLE <- tam.personfit(PCM.science_MMLE)
summary(person.fit.results_MMLE)
```

## Plot the Wright Map:

Next, we will create a Wright Map from our model results. This display will provide us with an overview of the distribution of person, item, and threshold parameters. We will create the plot using the WrightMap package function *IRT.WrightMap* on the model object (*PCM.science_MMLE*). 

For ease in interpretation, we will use the centered item and person locations that we calculated in this analysis. We need to specify these modified parameter estimates in the WrightMap function. The following code prepares the parameter estimates and plots the Wright Map using them.

```{r}
# Combine centered item estimates with thresholds:

n.items <- ncol(science.responses)

thresholds_MMLE <- matrix(data = NA, nrow = n.items, ncol = n.thresholds)

for(item.number in 1:n.items){
  for(tau in 1:n.thresholds){
  thresholds_MMLE[item.number, tau] <- centered.item.locations_MMLE[item.number] +
      tau.estimates_MMLE[item.number, tau]
  } 
}


#tau_labels <- paste("tau_", 1, sep = "")

thetas_MMLE <- person.locations_MMLE$theta_adjusted


# Plot the Variable Map
wrightMap(thetas = thetas_MMLE,
        thresholds = thresholds_MMLE,
         main.title = "Liking for Science Partial Credit Model Wright Map",
        show.thr.lab	= TRUE, dim.names = "",
        label.items.rows= 2)

```

In this *Wright Map* display, the results from the PCM analysis of the Liking for Science data are summarized graphically. The figure is organized as follows:

The left panel of the plot shows a histogram of respondent (children) locations on the logit scale that represents the latent variable. Units on the logit scale are shown on the far-right axis of the plot (labeled *Logits*). 

The large central panel of the plot shows the rating scale category threshold estimates specific to each item on the logit scale that represents the latent variable. Light grey diamond shapes show the logit scale location of the threshold estimates for each item, as labeled on the x-axis. Thresholds are labeled using tau symbols followed by an integer that shows the threshold number. In our example, $\tau$1 is the threshold between rating scale categories *x* = 0 and *x* = 1, and $\tau$2 is the threshold between rating scale categories *x* = 1 and *x* = 2.

Even though it is not appropriate to fully interpret item and person locations on the logit scale until there is evidence of acceptable model-data fit, we recommend examining the Wright Map during the preliminary stages of an item analysis to get a general sense of the model results and to identify any potential scoring or data entry errors.

A quick glance at the Wright Map suggests that, on average, the persons are located higher on the logit scale compared to the average item threshold locations. In addition, there appears to be a relatively wide spread of person and item locations on the logit scale, such that the Liking for Science questionnaire appears to be a useful tool for identifying differences in children's attitudes toward science activities as well as the difficulty to find each of the activities as favorable.

#*#*# Note: Stopped editing here on Feb 17 #*#*#*#*#




### R-Lab: Partial Credit Model with "eRm" package

```{r}
# Run the Partial Credit Model
PC_model <- PCM(PC_data_balanced)
# Check the result
summary(PC_model)
```

### Wright Map & Expected Response Curves & Item characteristic curves 
Wright Map or Variable Map
```{r}
# Plot the Variable Map
plotPImap(PC_model)
```

Item characteristic curves
```{r}
plotICC(PC_model, ask = FALSE)
```

### Examine item difficulty and threshold SEs

```{r}
### Examine item difficulty values:
item.estimates <- thresholds(PC_model)
item.estimates
item_difficulty <- item.estimates[["threshtable"]][["1"]]
item_difficulty

## Get threshold SEs values:
item.se <- item.estimates$se.thresh
item.se
```

### Examine Person locations (theta) and SEs
```{r}
# Standard errors for theta estimates:
person.locations.estimate <- person.parameter(PC_model)
summary(person.locations.estimate)
# Build a table for person locations
person_theta <- person.locations.estimate$theta.table
person_theta
```
### Exam the item and person fit statistics
```{r}
item.fit <- itemfit(person.locations.estimate)
item.fit
item.fit.table <- cbind(item.fit[["i.outfitMSQ"]],item.fit[["i.infitMSQ"]],item.fit[["i.infitMSQ"]],item.fit[["i.infitZ"]])
pfit <- personfit(person.locations.estimate)
pfit
person.fit.table <- cbind(pfit[["p.outfitMSQ"]],pfit[["p.outfitMSQ"]],pfit[["p.outfitMSQ"]],pfit[["p.outfitMSQ"]])
```

### Calculate the Person/Item Separation Reliability

As we mentioned in the previous chapter \@ref{Reliability}, Person/Item separation reliability is the estimate of how well we can differentiate individual items, persons, or other elements on the latent variable. Since not all the R packages include both separation reliability, the following is the example of calculating the Person/Item separation reliability.

We use the "braun_data" as an example:

#### Calculate the Item Separation Reliability

```{r}
# ===================================
# compute item separation reliability
# ===================================

# Get Item scores
ItemScores <- colSums(PC_data_balanced)

# Get Item SD
ItemSD <- apply(PC_data_balanced,2,sd)

# Calculate the se of the Item
ItemSE <- ItemSD/sqrt(length(ItemSD))

# compute the Observed Variance (also known as Total Person Variability or Squared Standard Deviation)
SSD.ItemScores <- var(ItemScores)

# compute the Mean Square Measurement error (also known as Model Error variance)
Item.MSE <- sum((ItemSE)^2) / length(ItemSE)

# compute the Item Separation Reliability
item.separation.reliability <- (SSD.ItemScores-Item.MSE) / SSD.ItemScores
item.separation.reliability
```

#### Calculate the Person Separation Reliability

```{r}
# ===================================
# compute person separation reliability
# ===================================

# Get Person scores
PersonScores <- rowSums(PC_data_balanced)

# Get Person SD
PersonSD <- apply(PC_data_balanced,1,sd)

# Calculate the se of the Person
PersonSE <- PersonSD/sqrt(length(PersonSD))

# compute the Observed Variance (also known as Total Person Variability or Squared Standard Deviation)
SSD.PersonScores <- var(PersonScores)

# compute the Mean Square Measurement error (also known as Model Error variance)
Person.MSE <- sum((PersonSE)^2) / length(PersonSE)

# compute the Person Separation Reliability
person.separation.reliability <- (SSD.PersonScores-Person.MSE) / SSD.PersonScores
person.separation.reliability
```

### Exercise
Can you plot the Standardized Residuals for our PC model? 
(Tips: You can use the R code from previous chapter, they're the same)

## Supplmentary Learning Materials 

[Braun, H. I. (1988). Understanding Scoring Reliability: Experiments in Calibrating Essay Readers. Journal of Educational and Behavioral Statistics, 13(1), 1–18.](http://doi.org/10.3102/10769986013001001) 

[Masters, G.N. A rasch model for partial credit scoring. Psychometrika 47, 149–174 (1982). https://doi.org/10.1007/BF02296272](https://link.springer.com/article/10.1007/BF02296272#citeas)









