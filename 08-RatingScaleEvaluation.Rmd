# Rating Scale Evaluation {#RS_Eval}

This chapter provides a basic overview of procedures for evaluating rating scales with Rasch models using R [@R-base]. We will use the *eRm* package [@eRm] and the *TAM* package [@TAM] for the analyses in this chapter. We use example data from an attitude survey to demonstrate procedures for evaluating rating scales using the Partial Credit Model (see Chapter 5).

## What is Rating Scale Evaluation?

When researchers evaluate rating scales, they examine the degree to which ordinal rating scales (e.g., Likert-type rating scales or rubrics in performance assessments) exhibit useful measurement characteristics. Essentially, rating scale evaluation involves examining rating scales for evidence that the categories are ordered as expected on the construct and that each category reflects a unique range of person locations on the construct. Several researchers (XXXX) have proposed practical guidelines for evaluating rating scales that analysts can use to identify issues and improve their rating scales. For example, Linacre (XXXX) presented a set of X guidelines for rating scales that can 


```{r message=FALSE,results='hide'}
##] Install & load the eRm package:
# install.packages("eRm")  # Install the pacakage if you do not have it
library("eRm")
```

## Rating Scale Analysis on Example Survey Data

For our first analysis, we will try conducting basic rating scale functioning analyses using some example survey data.

### Load and investigate the data set

```{r}
# load the example survey data into your working environment and generate a summary of the data
survey.responses <- read.csv("ratings.csv")
summary(survey.responses)
```

-	Examine the summary of the data to see if there are any missing values or other potentially problematic things we should know about before analyzing the data

Then, let's trim the data. Remove the “id” variable from the survey data because it’s not part of the item responses.

```{r}
survey.responses <- survey.responses[,-1]
# Use the apply function on table() function to check the frequencies of item responses in each category
apply(survey.responses,2,table)
```

* From these results, look to see if there are any categories with very few responses such that recoding might be warranted.

### Run the Partial Credit Model

```{r}
# Run the Partial Credit model on the item responses
PC_model <- PCM(survey.responses)
# After above code runs, the results are stored in an object called “PC_model”
```

### Check Categories for Monotonicity

Check category threshold estimates for non-decreasing (i.e., monotonic) ordering over increasing categories.

```{r}
# This is a function that finds the threshold values specific to each item and plots them.
responses <- survey.responses # update this if needed with the responses object
model.results <- PC_model # update this if needed with the model results object

for(item.number in 1:ncol(responses)){
  
  n.thresholds <-  length(table(responses[, item.number]))-1
  
  taus <- (thresholds(model.results))
  
  taus.item <- c(taus$threshpar[((item.number*n.thresholds)-(n.thresholds-1)): (item.number*n.thresholds)])*-1
  
  taus.item <- as.vector(unlist(taus.item))*-1
  
  plot(c(1:length(taus.item)), taus.item, ylab = "Threshold Location", xlab = "Threshold Number", 
       main = paste("Item ", item.number, "Threshold Locations"), axes = FALSE,
       ylim = c(floor(min(taus.item)), ceiling(max(taus.item))))
  
  axis(1, at = c(1:length(taus.item)))
  axis(2, at = c(floor(min(taus.item)): ceiling(max(taus.item))), 
       labels = c(floor(min(taus.item)): ceiling(max(taus.item))))
  
  lines(taus.item)
  
}

```

- Running the above code will generate plots for each item that show the estimated location for each threshold.

- Use the arrow buttons in the Plots window to navigate through the plots and examine the plots for evidence of non-decreasing threshold locations on the y-axis as the thresholds progress along the x-axis.

- You can also check the distance between the threshold locations for evidence of distinct categories.

You can export the item and threshold location estimates for use in reporting

```{r}
### Export overall item & threshold estimates:

item.estimates <- as.data.frame(taus$threshtable)

item.estimates <- round(item.estimates, digits = 2)

write.csv(item.estimates, file = "item_thresholds_PC_Model.csv")

```

### Examine Category Probability curves

Examine category probability curves for evidence of distinct and ordered categories. 

```{r}
# The function below that finds the threshold values specific to each item and plots them.
responses <- survey.responses # update to data object if using with other dataset!
model.results <- PC_model # update to model object if using with other dataset!
for(item.number in 1:ncol(survey.responses)){
  plotICC(model.results, item.subset = item.number)
}
```

The code above will produce category probability plots in the Plots window.

- Use the arrow buttons in the Plots window to navigate through the plots and examine the plots for evidence of:
  - Ordered category curves
  - Distinct (modal) categories

## Example APA-format Results section for rating scale analysis

  We analyzed 350 participants’ responses to a 15-item survey that included items with five ordered categories ($X$ = 0, 1, 2, 3, or 4) for evidence of useful rating scale category functioning. Specifically, following @Optimizing_RS and @Invar_measurement, we examined the survey item responses for evidence of directional orientation with the latent variable and distinct categories on the construct. So that we could consider category functioning specific to each item with the most diagnostic utility, we used the Partial Credit Model [@PCM] to analyze the data. We conducted the analyses using the Extended Rasch Models (“eRm”) package [@eRm] for R. 

![Table1:Item and Threshold Location Estimates from the Partial Credit Model](RSA_Table1.png)

  Participant responses to all 15 items included observations in all five rating scale categories. All of the items included at least 10 responses in each category except for the lowest rating scale category ($X$ = 0) for Item 3 (frequency = 8), Item 8 (frequency = 9), and Item 13 (frequency = 9). In a future administration of this survey, researchers may consider reducing the scale length for these items to avoid unstable estimates.
  
![Figure 1:Threshold Locations for Item3](RSA_Figure1.png)  
  
  First, we examined the estimated threshold locations for each item for evidence of directional orientation with the latent variable (i.e., non-decreasing locations as categories increase). Because all of the items included responses in five categories, there were four threshold estimates for each item ($τ_{i1}$,$τ_{i2}$,$τ_{i3}$,$τ_{i4}$). Accordingly, we examined whether $τ_{i1}$ ≤$ τ_{i2}$, $τ_{i2}$ ≤ $τ_{i3}$, and $τ_{i3}$ ≤ $τ_{i4}$ for each item. Table 1 includes the threshold estimates for each item, along with the overall item location estimate. For Item 3, the first and second threshold were disordered ($τ_{3,1}$ = -1.46,$τ_{3,2}$ = -1.68; see Figure 1). For all of the other items, the thresholds were ordered as expected given the ordinal rating scale categories.
  
![Figure 2: Category Probability Curves for Item 3](RSA_Figure2.png)    
  
  Next, we examined category probability curves for evidence that the rating scale categories were ordered as expected (non-decreasing threshold locations over increasing categories) and distinct for each item. Distinct categories imply that each rating scale category describes a meaningful range of locations on the logit scale that represents the construct. The categories appeared to function as expected for all of the items except Item 3 (see Figure 2). For this item, the first and second rating scale categories were disordered.
